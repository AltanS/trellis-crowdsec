# CrowdSec Scenario: Aggressive Crawl Detection (WordPress-compatible)
# Managed by Ansible - do not edit manually
#
# Supplements the Hub's http-crawl-non_statics scenario which fails on
# WordPress sites using pretty permalinks. The Hub scenario uses
# distinct: "evt.Parsed.file_name" — but WordPress URLs end with /
# (e.g. /bonus/, /tipps/match-prognose/), so file_name is always empty
# and the distinct filter collapses all requests into one bucket entry.
#
# This scenario uses distinct: "evt.Meta.http_path" instead, counting
# each unique URL path as a separate entry. Combined with a slower leak
# rate (5s vs 0.5s), it catches both fast scrapers (~9 req/sec, triggers
# in ~5s) and slow persistent scrapers (~0.5 req/sec, triggers in ~2min).
#
# A legitimate user browsing 15 pages/min (0.25/sec) would need to sustain
# that rate for 13+ minutes to trigger — not realistic browsing behavior.
#
# Excludes Ahrefs crawlers (AhrefsBot, AhrefsSiteAudit) — legitimate SEO tool.
# Googlebot/Bingbot are already excluded by crowdsecurity/whitelist-good-actors.

type: leaky
name: custom/aggressive-crawl
description: "Detect aggressive crawling on WordPress sites (pretty permalink compatible)"
filter: |
  evt.Meta.log_type in ['http_access-log', 'http_error-log'] and
  evt.Parsed.static_ressource == 'false' and
  evt.Parsed.verb in ['GET', 'HEAD'] and
  !(evt.Meta.http_user_agent contains 'Ahrefs')
distinct: "evt.Meta.http_path"
groupby: evt.Meta.source_ip
capacity: {{ crowdsec_scenario_aggressive_crawl_capacity }}
leakspeed: {{ crowdsec_scenario_aggressive_crawl_leakspeed }}
blackhole: {{ crowdsec_scenario_aggressive_crawl_blackhole }}
cache_size: 5
labels:
  service: http
  type: crawl
  remediation: true
